# 📚 Useful materials for Deep Learning and Machine Learning

딥러닝/머신러닝 자료 정리 repo입니다. 즐겨찾기로 저장하는 것도 귀찮고, 다른 분들과 많이 공유하기 위해 본 repo를 작성하게 되었습니다. 최대한 한국어로 된 자료를 담으려고 노력했으나 다들 아시다시피 많지는 않습니다. 한국어로 된 자료는 한국어로 썼고, 추천하는 자료는 볼드로 표시했습니다.

크게 material과 article이 있는데, material은 책, 강의 등의 자료로 reference로 삼기 좋습니다. article은 여러 주제에 대해 모았는데, 한 번 읽어보면 좋을 자료들입니다.

| Emoji | 설명 |
| :--- | :--- |
| 📚: Book | 책 |
| 👨‍🏫: Course | 강의 |
| 📑: Article | 단발성의, 읽어보면 좋을 듯한 자료를 의미. article은 특정 주제에 따라 분리해놨음 |
| 🗃️: Repo or Web | repository나 웹사이트 |



# 1. 👨‍💻 Programming

| 📚 Materials | link | 설명 |
| :--- | :--- | :--- |
| **리팩토링** | 📚 [(link)](http://www.yes24.com/Product/Goods/267290) | 코드를 짜는데 어떻게 클래스를 나누고 함수를 짜야될지 고민이 된다? 이 책을 볼 때가 된거임 |
| 중급파이썬: 파이썬 팁들 | 📚 [(link)](https://ddanggle.gitbooks.io/interpy-kr/content/) | 엄청 어렵지는 않으므로 초보를 벗어났다 싶을 때 필요한 것만 쏙쏙 봐도 될 것 같음 |
| 파이썬 클린코드 | 📚 [(link)](http://www.yes24.com/Product/Goods/69064790) | 그 클린코드 아님 |
| **Minimal and clean example implementations of data structures and algorithms in Python 3** | 🗃️ [(link)](https://github.com/keon/algorithms) | 매우 괜찮음. 직관적이고 간단한 문제들과 예제를 보여줌 |
| **기술 면접 대비 Tech Interview** | 🗃️ [(link)](https://github.com/keon/algorithms) | IT 기업 면접에 자주 나오는 질문과 그에 대한 답변 목록. 면접보기전에 봐야 함 |



# 2. 🧮 Linear Algebra & Statistics
| 📚 Materials | link | 설명 |
| :--- | :--- | :--- |
| Gilbert strang 교수 강의 정리 | 🗃️ [(link)](https://twlab.tistory.com/17) | 잘 모르는거 생길 때 와서 리뷰하면 좋음 |
| 모두를 위한 컨벡스 최적화 | 🗃️ [(link)](https://wikidocs.net/17202) | 모두의 연구소에서 진행한 스터디 |
| **공돌이의 수학정리 노트** | 🗃️ [(link)](https://angeloyeo.github.io/) | 수학 잘 모르는 사람은 어려운거 나오면 이거 보면서 정리하면 됨 |
| **ASDF 오터의 통계** | 📺 [(link)](https://www.youtube.com/channel/UCgqqOZRUcnSJHDOPHhIQCwg) | 수식없이 설명해주는 통계 |


# 3. 🤖 Machine Learning
| 📚 Materials | link | 설명 |
| :--- | :--- | :--- |
| **PRML** | 📚 [(link)](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) | 비숍책. 바이블 |
| **PRML 정리글** | 🗃️ [(link)](http://norman3.github.io/prml/?fbclid=IwAR2Rv0O_LG67Oa3rzJ9KnguRs9LiXuSYdfCSu60DzG0qxlpX6I4w61sHack) | PRML 한국어 정리글. 정리는 잘 되어있는데, 업데이트가 잘 안됨 |
| **Machine Learning: a Probabilistic Perspective** | 📚 [(link)](https://www.cs.ubc.ca/~murphyk/MLbook/) | 머피책. 바이블. 학부생이 좀 더 보기 편하고, generative model에 대해 잘 소개함 |
| The Elements of Statistical Learning | 📚 [(link)](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) | 통계학 측면에서의 machine learning. 좀 어렵다 |
| Introduction to statistical learning | 📚 [(link)](https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf) | 마찬가지로 통계학 관점에서의 책. 그러나 ESL보다 더 쉽다 |
| awesome-RecSys | 🗃️ [(link)](https://github.com/jihoo-kim/awesome-RecSys?fbclid=IwAR3V7QJXxXKzrW0EXcp4kwPcgKQtCUcHGrnY9K_ANQ1iPkkmWGTcLQzEFVo) | 추천시스템 관련 material 모음집 |
| CS229 | 👨‍🏫 [(link)](https://www.easyupclass.com/course/56/about) | 앤드류 응의 CS229 한글자막 |

| 📑 Generative model | link | 설명 |
| :--- | :--- | :--- |
| Variational Inference | 📑 [(link)](https://hyeongminlee.github.io/post/bnn003_vi/) | VAE 설명 |


# 4. 🕸️ Deep learning

| 📚 Materials | link | 설명 |
| :--- | :--- | :--- |
| Papers with code | 🗃️ [(link)](https://paperswithcode.com/method) | paper와 코드가 정리되어 있음. 처음보는 모델이 나올 땐 논문 통채로 읽지말고 일단 여기서 찾아보자 |
| Deep Learning Book | 📚 [(link)](https://www.deeplearningbook.org/) | 저자만 봐도 지림. 이름이 깔끔해서 오해하기 쉽지만 꽤나 어려움. 번역판은 질이 좋지 않음. 잘 찾아보면 pdf판 구할 수 있음 |
| Papers You Must Read | 📚 [(link)](https://www.notion.so/c3b3474d18ef4304b23ea360367a5137?v=5d763ad5773f44eb950f49de7d7671bd) | 고대 DSBA 연구실에서 작성한 논문 목록 |
| DEEP LEARNING NYU 강의 | 📚 [(link)](https://atcold.github.io/pytorch-Deep-Learning/) | 뉴욕대 딥러닝 강의. 얀르쿤이 함 |


# 5. NLP

| 📚 Materials | link | 설명 |
| :--- | :--- | :--- |
| 딥 러닝을 이용한 자연어 처리 입문 | 📚 [(link)](https://wikidocs.net/book/2155) | 막 큰 도움은 안 되지만 커리큘럼이 있다는것 자체가 좋음. 잘 모르면 이거 따라가면 될듯 |
| 김기현의 자연어 처리 딥러닝 캠프 | 📚 [(link)](https://github.com/songys/AwesomeKorean_Data) | 한국어로 된 파이토치/자연어처리 책 중 가장 괜찮은 책. 그러나 파이토치 초보가 보기도 어렵고, NLP 초보가 보기도 어렵다 |
| **CS224n** | 👨‍🏫 [(link)](https://web.stanford.edu/class/cs224n/) | 대가 Manning 교수의 강의. 직접 봐도 되고, 아니면 내가 스터디한 [레포](https://github.com/InhyeokYoo/CS224N)봐도 됨 😉 |
| Huggingface 데이터 셋 | 🗃️ [(link)](https://huggingface.co/nlp/viewer/?dataset=snli) | 허깅 페이스 데이터 셋 |
| 한국어 데이터 셋 모음 | 🗃️ [(link)](https://github.com/kh-kim/nlp_with_pytorch_examples) | 한국어 데이터 셋 목록 |
| Modern Deep Learning Techniques Applied to Natural Language Processing | 🗃️ [(link)](https://nlpoverview.com/) | 딥러닝 최신 글들에 대한 리뷰 및 정리 같은거 간단하게 되어 있음. 마지막 업데이트가 2020년 3월 25일 |
| Generalized Language Models | 🗃️ [(link)](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html) | Language model에 대한 소개글. CoVe부터 GPT-2까지 다양하게 있다. 마찬가지로 레퍼런스로 보기 좋음 |


| 📑 Papers | link | 설명 |
| :--- | :--- | :--- |
| Huggingface Awesome NLP Paper Discussions | 🗃️ [(link)](https://github.com/huggingface/awesome-papers) | 읽을 논문 선정하는 것도 일인데 고생하지 말고 그냥 얘네랑 같이 읽자 |
| Kakao: 2018-2020 NLU 연구 동향을 소개합니다 | 🗃️ [(link)](https://kakaobrain.com/blog/118) | 카카오에서 소개하는 NLU 연구 동향 |
| Deep-Generative-Models-for-Natural-Language-Processing | 🗃️ [(link)](https://github.com/FranxYao/Deep-Generative-Models-for-Natural-Language-Processing#Textbooks-and-Phd-Thesis) | Deep generative model 로드맵 |
| nlp-tutorial | 🗃️ [(link)](https://github.com/graykode/nlp-tutorial/blob/master/README.md) | NLP 기초적인 모델들 파이토치/텐서플로로 구현한 레포 |

## NMT

| 📚 Materials | link | 설명 |
| :--- | :--- | :--- |
| Back-Translation Review | 📑 [(link)](https://kh-kim.github.io/blog/2020/09/30/Back-Translation-Review.html) | 김기현씨 블로그 |
| Back Translation 정리: 번역기 성능 영혼까지 끌어모으기 | 📑 [(link)](https://dev-sngwn.github.io/2020-01-07-back-translation/) | 깔끔한 Back-translation 설명 |


# 6. Vision

- 📑 Articles: 이미지에서 CNN을 쓰는 이유 [(link)](https://medium.com/@seoilgun/cnn%EC%9D%98-stationarity%EC%99%80-locality-610166700979)
- 📚 Material: [Feature Detection and Description](https://docs.opencv.org/master/db/d27/tutorial_py_table_of_contents_feature2d.html)
    - feature detection과 description에 대해 설명하고 있음.

# Step by Step

| Data | link | 설명 |
| :--- | :--- | :--- |

| Training | link | 설명 |
| :--- | :--- | :--- |
| Backpropagation Through Time for Recurrent Neural Network| 📑 [(link)](https://mmuratarat.github.io/2019-02-07/bptt-of-rnn) | BPTT에 대한 설명과 수식 이해 |
| The Softmax function and its derivative | 📑 [(link)](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) | 소프트맥스와 이의 derivative 설명 |

| Hyperparamter tuning | link | 설명 |
| :--- | :--- | :--- |
| hyperparamter optimization | 📑 [(link)](https://m.blog.naver.com/laonple/220576650094) | 하이퍼 파라미터 최적화 방법 |
| Learning rate Decay의 종류 | 📑 [(link)](https://velog.io/@good159897/Learning-rate-Decay%EC%9D%98-%EC%A2%85%EB%A5%98) | Learning rate Decay 종류 살펴보기 |
| Pytorch Learning Rate Scheduler (러닝 레이트 스케쥴러) 정리 | 📑 [(link)](https://gaussian37.github.io/dl-pytorch-lr_scheduler/) | PyTorch learning rate scheduler 설명 |
| A CLOSER LOOK AT DEEP LEARNING HEURISTICS: LEARNING RATE RESTARTS, WARMUP AND DISTILLATION | 📑 [(link)](https://openreview.net/pdf?id=r14EOsCqKX) |  |
| Quick Tutorial: Using Bayesian optimization to tune your hyperparameters in PyTorch | 📑 [(link)](https://towardsdatascience.com/quick-tutorial-using-bayesian-optimization-to-tune-your-hyperparameters-in-pytorch-e9f74fc133c2) | Ax 사용하여 bayesian search하기 |
| Hyperparameter tuning with Ray Tune | 📑 [(link)](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html#full-training-function) | `ray tune` 이용하여 hyperparameter search하기 |


